# From RNNs to Transformers: A Curated Reading Path

This handout will help you deepen your understanding of how transformer models evolved from earlier sequence models like RNNs and LSTMs, why they were a breakthrough, and how they continue to develop.

---

## 1. Sequence Models: RNNs & LSTMs
**Goal:** Understand how we first handled sequences.

- ğŸ“– [Understanding LSTM Networks â€“ Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
- ğŸ“– *Deep Learning* (Goodfellow et al.) â€“ Chapter 10 covers RNNs and LSTMs.  

**Takeaway:** RNNs introduced temporal memory but struggled with long-range context and training stability.

---

## 2. The Limits of RNNs
**Goal:** Recognise why we needed something better.

- ğŸ“– [Stanford CS224n Lecture Notes (2019)](http://web.stanford.edu/class/cs224n/) â€“ see sequence models section.  

**Takeaway:** RNNs/LSTMs are sequential, slow to train, and still weak at long dependencies.

---

## 3. Attention Mechanisms
**Goal:** Discover the breakthrough idea before transformers.

- ğŸ“„ [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al., 2014)](https://arxiv.org/abs/1409.0473)  
- ğŸ“– [The Illustrated Attention Mechanism â€“ Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention/)  

**Takeaway:** Attention allows models to selectively focus on relevant parts of the sequence.

---

## 4. Transformers: The Breakthrough
**Goal:** Learn how attention replaced recurrence.

- ğŸ“„ [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)  
- ğŸ“– [The Illustrated Transformer â€“ Jay Alammar](http://jalammar.github.io/illustrated-transformer/)  
- ğŸ’» [The Annotated Transformer (Harvard NLP)](http://nlp.seas.harvard.edu/annotated-transformer/)  

**Takeaway:** Transformers use self-attention and parallelism to handle long-range dependencies efficiently.

---

## 5. Interactive Simulation
**Goal:** Explore how transformers work step by step.

- ğŸ•¹ï¸ [Transformer Visualisation â€“ Harvard NLP](http://nlp.seas.harvard.edu/annotated-transformer/) (interactive elements)  
- ğŸ•¹ï¸ [The Transformer from Scratch â€“ Peter Bloem](http://peterbloem.nl/blog/transformers)  
- ğŸ•¹ï¸ [Attention Playground â€“ Distill.pub](https://distill.pub/2016/augmented-rnns/#attention)  

**Takeaway:** These tools let you see attention weights and token interactions in real time.

---

## 6. Transformer Variants
**Goal:** See how the architecture evolved for different use cases.

- ğŸ“„ [BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805)  
- ğŸ“„ [Language Models are Unsupervised Multitask Learners (GPT-2 paper, Radford et al.)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
- ğŸ“– [The Illustrated BERT, ELMo, and co. â€“ Jay Alammar](http://jalammar.github.io/illustrated-bert/)  

**Takeaway:** Same building blocks (attention + feed-forward layers), but adapted:  
- Encoder-only (BERT) â†’ understanding  
- Decoder-only (GPT) â†’ generation  
- Encoder-decoder (T5/BART) â†’ flexible seq2seq

---

## 7. Scaling & Efficiency
**Goal:** Understand why modern models (GPT-4, LLaMA, etc.) are possible.

- ğŸ“„ [Scaling Laws for Neural Language Models (Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361)  
- ğŸ“– [Efficient Transformers â€“ HuggingFace Blog](https://huggingface.co/blog/efficient-transformers)  

**Takeaway:** Performance improves predictably with more data/parameters â€” but efficiency tricks are essential.

---

## 8. Hands-On Practice
**Goal:** Move from reading to experimenting.

- ğŸ’» [HuggingFace Transformers Course](https://huggingface.co/course/chapter1)  
- ğŸ’» [Papers with Code â€“ Transformers Collection](https://paperswithcode.com/method/transformer)  

**Takeaway:** Build, fine-tune, and experiment with real transformer models.

---

## Key Takeaways
- RNNs introduced memory â†’ LSTMs improved it â†’ but both struggled with long-range dependencies.  
- Attention let models â€œlook anywhereâ€ in the sequence â†’ a paradigm shift.  
- Transformers unlocked parallelism and scale â†’ foundation for todayâ€™s LLMs.  
- Modern progress = scaling + efficiency + variants (BERT, GPT, T5, etc.).  
- You learn best by combining **theory + visualisation + code**.

---
