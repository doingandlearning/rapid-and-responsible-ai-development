#!/usr/bin/env python3
"""
Section 8 Complete Solution: Production Deployment
Edinburgh University Vector Search System - Enterprise-Grade Implementation

This solution provides a complete, production-ready vector search system
with enterprise-level security, monitoring, scalability, and operational excellence.
"""

import os
import sys
import json
import time
import logging
import hashlib
import secrets
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
from contextlib import contextmanager
from functools import wraps

# Core dependencies
import psycopg
from psycopg_pool import ConnectionPool
import redis
import requests

# Flask web framework
from flask import Flask, request, jsonify, g
from flask_cors import CORS
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from flask_jwt_extended import JWTManager, create_access_token, verify_jwt_in_request, get_jwt_identity

# Monitoring and metrics
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/tmp/vector_search_production.log')
    ]
)

logger = logging.getLogger('production_vector_search')

# Prometheus metrics
REQUEST_COUNT = Counter('vector_search_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('vector_search_request_duration_seconds', 'Request duration')
ACTIVE_CONNECTIONS = Gauge('vector_search_active_connections', 'Active database connections')
SEARCH_QUERIES = Counter('vector_search_queries_total', 'Search queries', ['user_role', 'department'])
EMBEDDING_REQUESTS = Counter('embedding_requests_total', 'Embedding generation requests', ['status'])
CACHE_OPERATIONS = Counter('cache_operations_total', 'Cache operations', ['operation', 'status'])

@dataclass
class ProductionConfig:
    """Production system configuration."""
    
    # Database configuration
    db_host: str = os.getenv('DB_HOST', 'localhost')
    db_port: int = int(os.getenv('DB_PORT', '5432'))
    db_name: str = os.getenv('DB_NAME', 'pgvector')
    db_user: str = os.getenv('DB_USER', 'postgres')
    db_password: str = os.getenv('DB_PASSWORD', 'postgres')
    
    # Connection pooling
    db_pool_min_size: int = int(os.getenv('DB_POOL_MIN_SIZE', '5'))
    db_pool_max_size: int = int(os.getenv('DB_POOL_MAX_SIZE', '20'))
    
    # Redis configuration
    redis_host: str = os.getenv('REDIS_HOST', 'localhost')
    redis_port: int = int(os.getenv('REDIS_PORT', '6379'))
    redis_db: int = int(os.getenv('REDIS_DB', '0'))
    
    # Embedding service
    ollama_url: str = os.getenv('OLLAMA_URL', 'http://localhost:11434/api/embed')
    
    # Security
    jwt_secret_key: str = os.getenv('JWT_SECRET_KEY', 'dev-secret-change-in-production')
    flask_secret_key: str = os.getenv('FLASK_SECRET_KEY', 'dev-secret-change-in-production')
    
    # Rate limiting
    rate_limit_per_minute: int = int(os.getenv('RATE_LIMIT_PER_MINUTE', '60'))
    search_rate_limit: int = int(os.getenv('SEARCH_RATE_LIMIT_PER_MINUTE', '30'))
    
    # Cache settings
    cache_ttl_search: int = int(os.getenv('CACHE_TTL_SEARCH', '900'))  # 15 minutes
    cache_ttl_embedding: int = int(os.getenv('CACHE_TTL_EMBEDDING', '3600'))  # 1 hour
    
    # Performance settings
    max_results_per_query: int = int(os.getenv('MAX_RESULTS_PER_QUERY', '50'))
    embedding_timeout: int = int(os.getenv('EMBEDDING_TIMEOUT', '30'))
    query_timeout: int = int(os.getenv('QUERY_TIMEOUT', '60'))

@dataclass
class UserProfile:
    """User profile with Edinburgh-specific roles and permissions."""
    user_id: str
    username: str
    role: str  # student, staff, academic, admin
    department: Optional[str]
    campus: Optional[str]
    clearance_level: int
    rate_limit: int
    max_results: int

@dataclass
class SearchRequest:
    """Structured search request."""
    query: str
    filters: Dict[str, Any]
    user_profile: UserProfile
    config: Dict[str, Any]
    request_id: str

@dataclass  
class SearchResponse:
    """Structured search response."""
    results: List[Dict[str, Any]]
    count: int
    query: str
    response_time: float
    from_cache: bool
    request_id: str
    user_id: str
    timestamp: float

class ProductionSecurityManager:
    """Production-grade security management."""
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.audit_logger = self.setup_audit_logging()
        
        # Edinburgh University role configuration
        self.role_config = {
            'student': {
                'clearance_level': 2,
                'rate_limit': 30,
                'max_results': 20,
                'allowed_departments': None  # Can search any public content
            },
            'staff': {
                'clearance_level': 3,
                'rate_limit': 60,
                'max_results': 50,
                'allowed_departments': None  # Department-specific filtering applied
            },
            'academic': {
                'clearance_level': 4,
                'rate_limit': 100,
                'max_results': 100,
                'allowed_departments': None
            },
            'admin': {
                'clearance_level': 5,
                'rate_limit': 200,
                'max_results': 200,
                'allowed_departments': None
            }
        }
    
    def setup_audit_logging(self) -> logging.Logger:
        """Setup audit trail logging."""
        
        audit_logger = logging.getLogger('audit')
        audit_handler = logging.FileHandler('/tmp/vector_search_audit.log')
        
        audit_formatter = logging.Formatter(
            '{"timestamp": "%(asctime)s", "user_id": "%(user_id)s", '
            '"action": "%(action)s", "query": "%(query)s", '
            '"results_count": %(results_count)d, "ip": "%(ip)s", '
            '"session_id": "%(session_id)s", "success": %(success)s}'
        )
        
        audit_handler.setFormatter(audit_formatter)
        audit_logger.addHandler(audit_handler)
        audit_logger.setLevel(logging.INFO)
        
        return audit_logger
    
    def create_user_profile(self, user_data: Dict[str, Any]) -> UserProfile:
        """Create user profile from authentication data."""
        
        role = user_data.get('role', 'student')
        role_config = self.role_config.get(role, self.role_config['student'])
        
        return UserProfile(
            user_id=user_data.get('user_id', 'anonymous'),
            username=user_data.get('username', 'anonymous'),
            role=role,
            department=user_data.get('department'),
            campus=user_data.get('campus'),
            clearance_level=role_config['clearance_level'],
            rate_limit=role_config['rate_limit'],
            max_results=role_config['max_results']
        )
    
    def validate_search_permissions(self, user: UserProfile, filters: Dict[str, Any]) -> bool:
        """Validate user permissions for search request."""
        
        # Check clearance level
        requested_clearance = filters.get('max_clearance_level', user.clearance_level)
        if requested_clearance > user.clearance_level:
            return False
        
        # Department restrictions for sensitive content
        if requested_clearance >= 4 and not user.department:
            return False
        
        return True
    
    def sanitize_query(self, query: str) -> str:
        """Sanitize user input for security."""
        
        # Remove potentially dangerous characters
        import re
        query = re.sub(r'[<>"\'\\\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\xff]', '', query)
        
        # Limit length
        query = query[:500]
        
        # Remove SQL injection patterns (defense in depth)
        dangerous_patterns = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'CREATE', 'ALTER', 'EXEC']
        for pattern in dangerous_patterns:
            query = re.sub(f'\\b{pattern}\\b', '', query, flags=re.IGNORECASE)
        
        return query.strip()
    
    def log_search_activity(self, user: UserProfile, query: str, results_count: int, 
                          ip_address: str, success: bool) -> None:
        """Log search activity for audit trail."""
        
        self.audit_logger.info(
            "Search query executed",
            extra={
                'user_id': user.user_id,
                'action': 'search',
                'query': query[:100],  # Truncate for logging
                'results_count': results_count,
                'ip': ip_address,
                'session_id': f"{user.user_id}_{int(time.time())}",
                'success': success
            }
        )

class ProductionCacheManager:
    """Production-grade caching with Redis."""
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.redis_client = redis.Redis(
            host=config.redis_host,
            port=config.redis_port,
            db=config.redis_db,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=10,
            retry_on_timeout=True,
            health_check_interval=30
        )
        
        # Test Redis connection
        try:
            self.redis_client.ping()
            logger.info("Redis connection established")
        except Exception as e:
            logger.error(f"Redis connection failed: {e}")
    
    def generate_cache_key(self, prefix: str, **kwargs) -> str:
        """Generate consistent cache key."""
        
        key_data = f"{prefix}:{json.dumps(kwargs, sort_keys=True)}"
        
        if len(key_data) > 250:  # Redis key length limit
            key_hash = hashlib.sha256(key_data.encode()).hexdigest()
            return f"{prefix}:hash:{key_hash}"
        
        return key_data
    
    def get_cached_result(self, cache_key: str) -> Optional[Any]:
        """Retrieve cached result with error handling."""
        
        try:
            cached = self.redis_client.get(cache_key)
            if cached:
                CACHE_OPERATIONS.labels(operation='hit', status='success').inc()
                return json.loads(cached)
            else:
                CACHE_OPERATIONS.labels(operation='miss', status='success').inc()
                return None
                
        except Exception as e:
            CACHE_OPERATIONS.labels(operation='get', status='error').inc()
            logger.warning(f"Cache read failed for key {cache_key}: {e}")
            return None
    
    def cache_result(self, cache_key: str, result: Any, ttl: int) -> bool:
        """Cache result with TTL and error handling."""
        
        try:
            serialized = json.dumps(result)
            success = self.redis_client.setex(cache_key, ttl, serialized)
            
            if success:
                CACHE_OPERATIONS.labels(operation='set', status='success').inc()
                return True
            else:
                CACHE_OPERATIONS.labels(operation='set', status='error').inc()
                return False
                
        except Exception as e:
            CACHE_OPERATIONS.labels(operation='set', status='error').inc()
            logger.warning(f"Cache write failed for key {cache_key}: {e}")
            return False
    
    def invalidate_pattern(self, pattern: str) -> int:
        """Invalidate cache keys matching pattern."""
        
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                deleted = self.redis_client.delete(*keys)
                CACHE_OPERATIONS.labels(operation='invalidate', status='success').inc()
                return deleted
            return 0
            
        except Exception as e:
            CACHE_OPERATIONS.labels(operation='invalidate', status='error').inc()
            logger.error(f"Cache invalidation failed for pattern {pattern}: {e}")
            return 0

class ProductionEmbeddingService:
    """Production-grade embedding service with failover."""
    
    def __init__(self, config: ProductionConfig, cache_manager: ProductionCacheManager):
        self.config = config
        self.cache_manager = cache_manager
        self.session = requests.Session()
        
        # Configure session for reliability
        self.session.timeout = config.embedding_timeout
        self.session.headers.update({'Content-Type': 'application/json'})
        
    def get_embedding(self, text: str, use_cache: bool = True) -> Optional[List[float]]:
        """Get embedding with caching and retry logic."""
        
        # Check cache first
        if use_cache:
            cache_key = self.cache_manager.generate_cache_key('embedding', text=text)
            cached_embedding = self.cache_manager.get_cached_result(cache_key)
            if cached_embedding:
                return cached_embedding
        
        # Generate embedding with retry
        for attempt in range(3):
            try:
                start_time = time.time()
                
                response = self.session.post(
                    self.config.ollama_url,
                    json={
                        'model': 'bge-m3',
                        'input': text
                    },
                    timeout=self.config.embedding_timeout
                )
                
                response.raise_for_status()
                
                embedding_data = response.json()
                embedding = embedding_data['embeddings'][0]
                
                # Cache successful embedding
                if use_cache:
                    self.cache_manager.cache_result(
                        cache_key, 
                        embedding, 
                        self.config.cache_ttl_embedding
                    )
                
                generation_time = time.time() - start_time
                EMBEDDING_REQUESTS.labels(status='success').inc()
                
                logger.debug(f"Embedding generated in {generation_time:.3f}s (attempt {attempt + 1})")
                return embedding
                
            except Exception as e:
                EMBEDDING_REQUESTS.labels(status='error').inc()
                logger.warning(f"Embedding attempt {attempt + 1} failed: {e}")
                
                if attempt < 2:  # Wait before retry
                    time.sleep(0.5 * (attempt + 1))
                else:
                    logger.error(f"All embedding attempts failed for text: {text[:50]}...")
        
        return None

class ProductionDatabaseManager:
    """Production-grade database operations with connection pooling."""
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        
        # Create connection pool
        self.pool = ConnectionPool(
            f"postgresql://{config.db_user}:{config.db_password}@"
            f"{config.db_host}:{config.db_port}/{config.db_name}",
            min_size=config.db_pool_min_size,
            max_size=config.db_pool_max_size,
            open=True
        )
        
        logger.info(f"Database connection pool created: {config.db_pool_min_size}-{config.db_pool_max_size} connections")
    
    @contextmanager
    def get_connection(self):
        """Get database connection from pool."""
        
        conn = None
        try:
            conn = self.pool.getconn()
            ACTIVE_CONNECTIONS.inc()
            yield conn
            
        except Exception as e:
            logger.error(f"Database connection error: {e}")
            if conn:
                conn.rollback()
            raise
            
        finally:
            if conn:
                ACTIVE_CONNECTIONS.dec()
                self.pool.putconn(conn)
    
    def execute_search_query(self, query_embedding: List[float], filters: Dict[str, Any], 
                           config: Dict[str, Any]) -> List[Tuple]:
        """Execute optimized search query with filters."""
        
        with self.get_connection() as conn:
            cur = conn.cursor()
            
            # Build dynamic query based on filters
            where_conditions = ["embedding <=> %s::vector < %s"]
            params = [query_embedding, config.get('similarity_threshold', 0.4)]
            
            # Add metadata filters
            if filters.get('department'):
                where_conditions.append("metadata->>'department' = %s")
                params.append(filters['department'])
            
            if filters.get('campus'):
                where_conditions.append("metadata->>'campus' = %s")
                params.append(filters['campus'])
            
            if filters.get('doc_type'):
                where_conditions.append("metadata->>'doc_type' = %s")
                params.append(filters['doc_type'])
            
            if filters.get('min_priority'):
                where_conditions.append("(metadata->>'priority')::int >= %s")
                params.append(filters['min_priority'])
            
            if filters.get('max_clearance_level'):
                where_conditions.append("(metadata->>'clearance_level')::int <= %s")
                params.append(filters['max_clearance_level'])
            
            # Build scoring expression
            scoring_components = [
                f"(1 - (embedding <=> %s::vector)) * {config.get('similarity_weight', 0.6)}",
                f"LEAST((metadata->>'priority')::float / 5.0, 1.0) * {config.get('priority_weight', 0.2)}",
                f"LEAST((metadata->>'view_count')::float / 5000.0, 1.0) * {config.get('popularity_weight', 0.1)}"
            ]
            
            # Department bonus
            if filters.get('user_department'):
                scoring_components.append(
                    f"CASE WHEN metadata->>'department' = %s THEN {config.get('department_weight', 0.1)} ELSE 0.0 END"
                )
                params.append(filters['user_department'])
            
            scoring_expression = f"({' + '.join(scoring_components)})"
            
            # Add query embedding for scoring
            params.insert(-1 if filters.get('user_department') else len(params), query_embedding)
            
            # Final query
            sql = f"""
                SELECT 
                    document_title,
                    section_title,
                    text,
                    page_number,
                    metadata,
                    1 - (embedding <=> %s::vector) as similarity,
                    {scoring_expression} as combined_score
                FROM document_chunks 
                WHERE {' AND '.join(where_conditions)}
                ORDER BY combined_score DESC
                LIMIT %s
            """
            
            # Add parameters for similarity calculation and limit
            params.insert(-1, query_embedding)  # For similarity calculation
            params.append(min(config.get('max_results', 10), 50))  # Limit results
            
            cur.execute(sql, params)
            return cur.fetchall()

class ProductionVectorSearchSystem:
    """Enterprise-grade vector search system."""
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.security_manager = ProductionSecurityManager(config)
        self.cache_manager = ProductionCacheManager(config)
        self.embedding_service = ProductionEmbeddingService(config, self.cache_manager)
        self.database_manager = ProductionDatabaseManager(config)
        
        logger.info("Production vector search system initialized")
    
    def process_search_request(self, request: SearchRequest) -> SearchResponse:
        """Process search request with full production pipeline."""
        
        start_time = time.time()
        request_id = request.request_id
        
        try:
            # Security validation
            if not self.security_manager.validate_search_permissions(request.user_profile, request.filters):
                raise ValueError("Insufficient permissions for requested search")
            
            # Sanitize query
            sanitized_query = self.security_manager.sanitize_query(request.query)
            
            # Check cache first
            cache_key = self.cache_manager.generate_cache_key(
                'search',
                query=sanitized_query,
                filters=request.filters,
                config=request.config,
                user_role=request.user_profile.role
            )
            
            cached_response = self.cache_manager.get_cached_result(cache_key)
            if cached_response:
                cached_response['from_cache'] = True
                cached_response['request_id'] = request_id
                cached_response['response_time'] = time.time() - start_time
                
                # Log cached request
                self.security_manager.log_search_activity(
                    request.user_profile, 
                    sanitized_query, 
                    cached_response['count'],
                    'cached',
                    True
                )
                
                SEARCH_QUERIES.labels(
                    user_role=request.user_profile.role,
                    department=request.user_profile.department or 'none'
                ).inc()
                
                return SearchResponse(**cached_response)
            
            # Generate embedding
            query_embedding = self.embedding_service.get_embedding(sanitized_query)
            if not query_embedding:
                raise ValueError("Failed to generate query embedding")
            
            # Apply user-specific filters
            search_filters = request.filters.copy()
            search_filters['max_clearance_level'] = min(
                search_filters.get('max_clearance_level', request.user_profile.clearance_level),
                request.user_profile.clearance_level
            )
            
            if request.user_profile.department:
                search_filters['user_department'] = request.user_profile.department
            
            # Execute search
            raw_results = self.database_manager.execute_search_query(
                query_embedding, 
                search_filters, 
                request.config
            )
            
            # Format results
            results = []
            for row in raw_results:
                (doc_title, section, text, page_num, metadata, 
                 similarity, combined_score) = row
                
                results.append({
                    'document_title': doc_title,
                    'section_title': section,
                    'text': text,
                    'page_number': page_num or 0,
                    'metadata': json.loads(metadata) if metadata else {},
                    'similarity': similarity,
                    'combined_score': combined_score
                })
            
            response_data = {
                'results': results,
                'count': len(results),
                'query': sanitized_query,
                'response_time': time.time() - start_time,
                'from_cache': False,
                'request_id': request_id,
                'user_id': request.user_profile.user_id,
                'timestamp': time.time()
            }
            
            # Cache successful response
            if results:
                self.cache_manager.cache_result(
                    cache_key, 
                    response_data, 
                    self.config.cache_ttl_search
                )
            
            # Log successful search
            self.security_manager.log_search_activity(
                request.user_profile,
                sanitized_query,
                len(results),
                'direct',
                True
            )
            
            SEARCH_QUERIES.labels(
                user_role=request.user_profile.role,
                department=request.user_profile.department or 'none'
            ).inc()
            
            return SearchResponse(**response_data)
            
        except Exception as e:
            # Log failed search
            self.security_manager.log_search_activity(
                request.user_profile,
                request.query,
                0,
                'error',
                False
            )
            
            logger.error(f"Search request {request_id} failed: {str(e)}")
            raise

class ProductionHealthMonitor:
    """Production system health monitoring."""
    
    def __init__(self, config: ProductionConfig, search_system: ProductionVectorSearchSystem):
        self.config = config
        self.search_system = search_system
    
    def check_system_health(self) -> Dict[str, Any]:
        """Comprehensive system health check."""
        
        health_status = {
            'database': self.check_database_health(),
            'embedding_service': self.check_embedding_service_health(),
            'cache': self.check_cache_health(),
            'search_functionality': self.check_search_functionality()
        }
        
        overall_healthy = all(
            component['healthy'] for component in health_status.values()
        )
        
        return {
            'overall_healthy': overall_healthy,
            'timestamp': time.time(),
            'components': health_status
        }
    
    def check_database_health(self) -> Dict[str, Any]:
        """Check database connectivity and performance."""
        
        start_time = time.time()
        
        try:
            with self.search_system.database_manager.get_connection() as conn:
                cur = conn.cursor()
                
                # Test basic connectivity
                cur.execute('SELECT 1')
                
                # Test vector operations
                cur.execute('SELECT vector_dims(\'[1,2,3]\'::vector)')
                
                # Check document availability
                cur.execute('SELECT COUNT(*) FROM document_chunks WHERE embedding IS NOT NULL')
                chunk_count = cur.fetchone()[0]
                
                response_time = time.time() - start_time
                
                return {
                    'healthy': chunk_count > 0,
                    'response_time': response_time,
                    'chunk_count': chunk_count,
                    'pool_size': self.search_system.database_manager.pool.size
                }
                
        except Exception as e:
            return {
                'healthy': False,
                'response_time': time.time() - start_time,
                'error': str(e)
            }
    
    def check_embedding_service_health(self) -> Dict[str, Any]:
        """Check embedding service availability."""
        
        start_time = time.time()
        
        try:
            # Test embedding generation
            test_embedding = self.search_system.embedding_service.get_embedding(
                "health check test", 
                use_cache=False
            )
            
            response_time = time.time() - start_time
            
            return {
                'healthy': test_embedding is not None,
                'response_time': response_time,
                'embedding_length': len(test_embedding) if test_embedding else 0
            }
            
        except Exception as e:
            return {
                'healthy': False,
                'response_time': time.time() - start_time,
                'error': str(e)
            }
    
    def check_cache_health(self) -> Dict[str, Any]:
        """Check Redis cache functionality."""
        
        start_time = time.time()
        
        try:
            # Test cache operations
            cache_manager = self.search_system.cache_manager
            
            test_key = f"health_check_{int(time.time())}"
            test_value = {"test": "health_check"}
            
            # Test write
            cache_manager.cache_result(test_key, test_value, 60)
            
            # Test read
            retrieved = cache_manager.get_cached_result(test_key)
            
            # Cleanup
            cache_manager.redis_client.delete(test_key)
            
            response_time = time.time() - start_time
            
            return {
                'healthy': retrieved == test_value,
                'response_time': response_time,
                'connection_active': True
            }
            
        except Exception as e:
            return {
                'healthy': False,
                'response_time': time.time() - start_time,
                'error': str(e)
            }
    
    def check_search_functionality(self) -> Dict[str, Any]:
        """Check end-to-end search functionality."""
        
        start_time = time.time()
        
        try:
            # Create test user profile
            test_user = UserProfile(
                user_id='health_check',
                username='health_check',
                role='staff',
                department='IT Services',
                campus='Central Campus',
                clearance_level=3,
                rate_limit=100,
                max_results=10
            )
            
            # Create test search request
            test_request = SearchRequest(
                query='health check test query',
                filters={},
                user_profile=test_user,
                config={'similarity_threshold': 0.3, 'max_results': 1},
                request_id=f'health_check_{int(time.time())}'
            )
            
            # Execute search
            response = self.search_system.process_search_request(test_request)
            
            response_time = time.time() - start_time
            
            return {
                'healthy': True,
                'response_time': response_time,
                'results_returned': response.count,
                'end_to_end_functional': True
            }
            
        except Exception as e:
            return {
                'healthy': False,
                'response_time': time.time() - start_time,
                'error': str(e)
            }

# Flask Application
def create_production_app(config: ProductionConfig) -> Flask:
    """Create production Flask application."""
    
    app = Flask(__name__)
    app.config['SECRET_KEY'] = config.flask_secret_key
    app.config['JWT_SECRET_KEY'] = config.jwt_secret_key
    app.config['JWT_ACCESS_TOKEN_EXPIRES'] = timedelta(hours=8)
    
    # Initialize extensions
    CORS(app)
    jwt = JWTManager(app)
    
    # Rate limiting
    limiter = Limiter(
        app,
        key_func=get_remote_address,
        default_limits=[f"{config.rate_limit_per_minute} per minute"]
    )
    
    # Initialize production systems
    search_system = ProductionVectorSearchSystem(config)
    health_monitor = ProductionHealthMonitor(config, search_system)
    
    @app.before_request
    def before_request():
        """Request preprocessing."""
        g.start_time = time.time()
        g.request_id = secrets.token_urlsafe(16)
        
        REQUEST_COUNT.labels(
            method=request.method,
            endpoint=request.endpoint or 'unknown',
            status='started'
        ).inc()
    
    @app.after_request
    def after_request(response):
        """Request postprocessing."""
        
        response_time = time.time() - g.get('start_time', time.time())
        REQUEST_DURATION.observe(response_time)
        
        REQUEST_COUNT.labels(
            method=request.method,
            endpoint=request.endpoint or 'unknown',
            status=str(response.status_code)
        ).inc()
        
        return response
    
    @app.route('/health', methods=['GET'])
    def health_check():
        """System health check endpoint."""
        
        try:
            health_status = health_monitor.check_system_health()
            status_code = 200 if health_status['overall_healthy'] else 503
            return jsonify(health_status), status_code
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return jsonify({
                'overall_healthy': False,
                'error': str(e),
                'timestamp': time.time()
            }), 503
    
    @app.route('/metrics', methods=['GET'])
    def metrics():
        """Prometheus metrics endpoint."""
        return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}
    
    @app.route('/api/search', methods=['POST'])
    @limiter.limit(f"{config.search_rate_limit} per minute")
    def search_documents():
        """Main search endpoint."""
        
        try:
            # Parse request
            data = request.get_json()
            if not data or 'query' not in data:
                return jsonify({'error': 'Query parameter required'}), 400
            
            query = data.get('query', '').strip()
            if len(query) < 3:
                return jsonify({'error': 'Query must be at least 3 characters'}), 400
            
            # Create user profile (in production, this would come from JWT token)
            user_data = data.get('user', {
                'user_id': 'demo_user',
                'username': 'demo',
                'role': 'staff',
                'department': 'IT Services',
                'campus': 'Central Campus'
            })
            
            user_profile = search_system.security_manager.create_user_profile(user_data)
            
            # Create search request
            search_request = SearchRequest(
                query=query,
                filters=data.get('filters', {}),
                user_profile=user_profile,
                config=data.get('config', {}),
                request_id=g.request_id
            )
            
            # Process search
            search_response = search_system.process_search_request(search_request)
            
            return jsonify(asdict(search_response))
            
        except Exception as e:
            logger.error(f"Search request failed: {str(e)}")
            return jsonify({
                'error': 'Search failed',
                'message': str(e),
                'request_id': g.request_id
            }), 500
    
    @app.route('/api/admin/stats', methods=['GET'])
    @limiter.limit("10 per minute")
    def admin_stats():
        """Administrative statistics endpoint."""
        
        try:
            # Get cache statistics
            cache_info = search_system.cache_manager.redis_client.info()
            
            stats = {
                'database': {
                    'pool_size': search_system.database_manager.pool.size,
                    'active_connections': ACTIVE_CONNECTIONS._value._value
                },
                'cache': {
                    'used_memory': cache_info.get('used_memory_human', 'unknown'),
                    'connected_clients': cache_info.get('connected_clients', 0),
                    'keyspace_hits': cache_info.get('keyspace_hits', 0),
                    'keyspace_misses': cache_info.get('keyspace_misses', 0)
                },
                'system': {
                    'uptime': time.time() - app.config.get('start_time', time.time()),
                    'requests_total': REQUEST_COUNT._value.sum(),
                    'avg_response_time': REQUEST_DURATION._value.sum() / max(REQUEST_DURATION._value.count(), 1)
                }
            }
            
            return jsonify(stats)
            
        except Exception as e:
            return jsonify({'error': 'Failed to retrieve stats', 'message': str(e)}), 500
    
    @app.route('/api/admin/cache/clear', methods=['POST'])
    @limiter.limit("5 per minute")
    def clear_cache():
        """Clear system cache (admin only)."""
        
        try:
            # In production, this would require admin authentication
            pattern = request.json.get('pattern', '*') if request.json else '*'
            
            if pattern == '*':
                # Clear all cache
                search_system.cache_manager.redis_client.flushdb()
                message = "All cache cleared"
            else:
                # Clear pattern-specific cache
                deleted = search_system.cache_manager.invalidate_pattern(pattern)
                message = f"Cleared {deleted} keys matching pattern: {pattern}"
            
            return jsonify({
                'message': message,
                'timestamp': time.time(),
                'success': True
            })
            
        except Exception as e:
            return jsonify({
                'error': 'Failed to clear cache',
                'message': str(e)
            }), 500
    
    # Error handlers
    @app.errorhandler(429)
    def ratelimit_handler(e):
        return jsonify({
            'error': 'Rate limit exceeded',
            'message': str(e.description),
            'retry_after': e.retry_after
        }), 429
    
    @app.errorhandler(500)
    def internal_error(error):
        logger.error(f"Internal server error: {error}")
        return jsonify({
            'error': 'Internal server error',
            'request_id': g.get('request_id', 'unknown')
        }), 500
    
    # Store startup time for uptime calculation
    app.config['start_time'] = time.time()
    
    return app

def run_production_server():
    """Run production server with proper configuration."""
    
    print("ðŸš€ EDINBURGH UNIVERSITY VECTOR SEARCH - PRODUCTION SERVER")
    print("=" * 80)
    
    # Load configuration
    config = ProductionConfig()
    
    # Create application
    app = create_production_app(config)
    
    print("âœ… Production server initialized")
    print(f"ðŸ”’ Security: JWT authentication enabled")
    print(f"âš¡ Performance: Connection pooling {config.db_pool_min_size}-{config.db_pool_max_size}")
    print(f"ðŸ”„ Caching: Redis at {config.redis_host}:{config.redis_port}")
    print(f"ðŸ” Embedding: Ollama at {config.ollama_url}")
    print(f"â±ï¸  Rate limiting: {config.rate_limit_per_minute}/min general, {config.search_rate_limit}/min search")
    
    print("\nðŸ“Š Available endpoints:")
    print("  ðŸ” POST /api/search - Main search endpoint")
    print("  â¤ï¸  GET /health - System health check") 
    print("  ðŸ“ˆ GET /metrics - Prometheus metrics")
    print("  ðŸ”§ GET /api/admin/stats - System statistics")
    print("  ðŸ—‘ï¸  POST /api/admin/cache/clear - Clear cache")
    
    print("\nðŸŽ¯ Starting production server...")
    print("=" * 80)
    
    # Run with Gunicorn in production, Flask dev server for testing
    if os.getenv('FLASK_ENV') == 'production':
        # Production deployment would use Gunicorn
        print("âš ï¸  Use Gunicorn for production deployment:")
        print("gunicorn --bind 0.0.0.0:5100 --workers 4 --timeout 60 production_system:app")
    else:
        # Development/testing
        app.run(host='0.0.0.0', port=5100, debug=False, threaded=True)

if __name__ == "__main__":
    """
    Production vector search system for Edinburgh University.
    
    This system provides enterprise-grade capabilities including:
    - High-availability database connection pooling
    - Production-ready caching with Redis
    - Comprehensive security and access control
    - Full monitoring and health checking
    - Rate limiting and abuse protection
    - Structured logging and audit trails
    - Prometheus metrics integration
    - Edinburgh-specific role and department handling
    """
    
    try:
        run_production_server()
        
    except KeyboardInterrupt:
        print("\n\nðŸ›‘ Shutting down production server...")
        
    except Exception as e:
        logger.error(f"Production server startup failed: {e}")
        sys.exit(1)