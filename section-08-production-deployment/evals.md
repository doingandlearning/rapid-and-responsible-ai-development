# üìè Guide to Evaluating LLM/RAG Workflows

## 1. Why Evals Matter

A RAG pipeline that ‚Äúseems to work‚Äù in demos can fail badly in production:

* **Hallucinations** (LLM makes up facts).
* **Irrelevant retrievals** (wrong chunks).
* **Bias / fairness issues**.
* **Latency** that breaks user expectations.

Evaluations give you **ground truth checks** and **continuous monitoring** so you can iterate systematically.

---

## 2. Levels of Evaluation

### A) Retrieval Evaluation

* **Goal:** Do we retrieve the right documents/chunks?
* **Metrics:**

  * **Recall\@k** ‚Äì proportion of relevant docs found within top-k results.
  * **Precision\@k** ‚Äì proportion of retrieved docs that are relevant.
  * **MRR (Mean Reciprocal Rank)** ‚Äì how far down the list the first relevant result is.
* **Manual setup:** Label a set of queries with expected docs ‚Üí compare retrieval outputs.
* **Automated trick:** If docs have structured metadata, use it as weak supervision (‚Äúall fee queries should surface finance docs‚Äù).

---

### B) Generation Evaluation

* **Goal:** Is the answer useful, accurate, and well-grounded?
* **Metrics:**

  * **Faithfulness / Groundedness** ‚Äì % of claims that are directly supported by retrieved docs.
  * **Answer relevance** ‚Äì does it address the query intent?
  * **Citation quality** ‚Äì are correct sources cited?
* **Methods:**

  * LLM-as-judge ‚Äì use a stronger model to rate outputs against guidelines.
  * Human annotation ‚Äì gold standard but expensive.
  * QA pairs ‚Äì compare generated answers with reference answers.

---

### C) System/Operational Evaluation

* **Latency** ‚Äì average time to retrieve + generate.
* **Cost per query** ‚Äì embedding + completion costs.
* **Robustness** ‚Äì measure under noisy input (typos, paraphrasing).
* **Bias/Fairness** ‚Äì differential performance across groups (e.g. domestic vs international students).

---

## 3. SaaS / Platform Options

Several tools provide evaluation dashboards & pipelines:

* **[TruLens](https://www.trulens.org/)**

  * Open-source + hosted options.
  * Provides ‚Äúfaithfulness, relevance, coherence‚Äù scores.
  * Easy integration with LangChain, LlamaIndex.

* **[Ragas](https://docs.ragas.io/)**

  * Purpose-built for RAG evaluation.
  * Focus on retrieval metrics + generation groundedness.
  * Integrates with HuggingFace + LangChain.

* **[LangSmith (LangChain)](https://www.langchain.com/langsmith)**

  * SaaS for tracing + evaluation.
  * Supports datasets, run tracking, LLM-as-judge evals.

* **[Arize Phoenix](https://phoenix.arize.com/)**

  * Observability for LLM apps.
  * Emphasis on tracing + dataset evaluation.

* **Others:** Weights & Biases LLMOps, PromptLayer, Humanloop, Helicone (for logging + basic evals).

---

## 4. Building Your Own Lightweight Evals

### Step 1: Collect Eval Dataset

* 50‚Äì200 **representative queries**.
* Label expected answers or at least expected sources.
* Include ‚Äúhard‚Äù cases: ambiguous, multi-hop, edge scenarios.

### Step 2: Run Retrieval Checks

* Store for each query:

  * Retrieved docs/chunks.
  * Which ground-truth docs appear in top-k.
* Calculate Recall\@k, Precision\@k, MRR.

### Step 3: Run Generation Checks

* For each query:

  * Prompt an LLM judge (e.g. GPT-4 or Claude) with:

    > ‚ÄúGiven the retrieved documents and the generated answer, rate:
    >
    > 1. Faithfulness (0‚Äì5), 2. Relevance (0‚Äì5), 3. Citation quality (0‚Äì5).‚Äù
* Aggregate scores across dataset.

### Step 4: Monitor in Production

* Log queries + answers + retrieved sources.
* Periodically sample and run them through the eval pipeline.
* Track latency, cost, and failure modes (empty retrieval, API errors).

---

## 5. Example: Minimal Eval Script (Python)

```python
import openai
from sklearn.metrics import precision_score, recall_score

# retrieval eval
def eval_retrieval(ground_truth, retrieved):
    gt_set, ret_set = set(ground_truth), set(retrieved)
    precision = len(gt_set & ret_set) / len(ret_set) if ret_set else 0
    recall = len(gt_set & ret_set) / len(gt_set) if gt_set else 0
    return precision, recall

# generation eval (LLM-as-judge)
def eval_generation(answer, context, question):
    prompt = f"""
    Question: {question}
    Answer: {answer}
    Retrieved Context: {context}

    Rate on 0‚Äì5 scale:
    1. Faithfulness (answer supported by context)
    2. Relevance (answer addresses question)
    3. Citation quality
    Respond as JSON with keys faithfulness, relevance, citation.
    """
    res = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message["content"]
```

---

## 6. Practical Tips

* Start with **small eval sets** (50 queries) ‚Üí scale later.
* Always include **edge cases** (negation queries, typos, multilingual).
* Combine **automated LLM-as-judge** with **spot-check human reviews**.
* Track **trends over time** (regression tests when you retrain or re-index).
* Don‚Äôt over-optimise one metric; balance faithfulness, recall, latency, cost.

---

‚úÖ With this approach you can either:

* Use **SaaS eval platforms** (faster, richer dashboards).
* Or build your own **lean eval pipeline** using JSON datasets, a judge LLM, and simple metrics.
